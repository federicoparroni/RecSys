{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/giovanni/Desktop/RecSys')\n",
    "import data.data as data\n",
    "import inout.importexport as imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import math\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy.sparse import load_npz\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the complete dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the recommendations from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_recs = imp.importcsv('reranking/train_als_4coll_xgboost25_23-58-10.csv', check_len=-1)\n",
    "\n",
    "print(raw_recs[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explode each row into multiple rows (one per interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recs_tracks = []\n",
    "for rec in raw_recs:\n",
    "    playlist_id = rec[0]\n",
    "    for t in rec[1:]:\n",
    "        recs_tracks.append([playlist_id, t])\n",
    "recs_df = pd.DataFrame(recs_tracks, columns=['playlist_id','track_id'])\n",
    "\n",
    "print(recs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append the 'profile_length' column to the recommendation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = data.get_target_playlists()\n",
    "targetURM = data.get_urm_train_1()[target_ids]\n",
    "user_profile_lengths = np.array(targetURM.sum(axis=1)).flatten()\n",
    "profile_lengths_df = pd.DataFrame({'playlist_id': target_ids, 'profile_length': user_profile_lengths})\n",
    "\n",
    "print(profile_lengths_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec_lengths_df = recs_df.merge(profile_lengths_df, on='playlist_id')\n",
    "print(rec_lengths_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popularity feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = data.get_playlists_df()\n",
    "popularity = df.groupby(['track_id']).size().reset_index(name='popularity')\n",
    "print(popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec_pop_df = rec_lengths_df.join(popularity.set_index('track_id'), on='track_id')\n",
    "print(rec_pop_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append the 'label' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urm_test = data.get_urm_test_1()\n",
    "test_labels = []\n",
    "\n",
    "last_playlist_id = -1\n",
    "for idx,row in recs_df.iterrows():\n",
    "    current_playlist_id = row['playlist_id']\n",
    "    track_id = row['track_id']\n",
    "    # cache the row of the urm test if same playlist of the previous iteration\n",
    "    if not current_playlist_id == last_playlist_id:\n",
    "        # tracks ids in the t row of urm test\n",
    "        tracks_ids = urm_test.getrow(current_playlist_id).nonzero()[1]\n",
    "        last_playlist_id = current_playlist_id\n",
    "    \n",
    "    test_labels.append(1 if track_id in tracks_ids else 0)\n",
    "\n",
    "test_labels_df = pd.DataFrame({'label': test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec_label_df = pd.concat([rec_pop_df, test_labels_df], axis=1)\n",
    "print(rec_label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [load_npz('raw_data/saved_r_hat/4_collaborative_l2.npz'), load_npz('raw_data/saved_r_hat/als_l2.npz')]\n",
    "scores = [[], []]\n",
    "n = len(matrix)\n",
    "for idx,row in rec_label_df.iterrows():\n",
    "    for i in range(len(matrix)):\n",
    "        scores[i].append(matrix[i][row['playlist_id'], row['track_id']])\n",
    "\n",
    "dfn1 = pd.DataFrame({'score_4_coll': scores[0]/max(scores[0])})\n",
    "dfn2 = pd.DataFrame({'score_als': scores[1]/max(scores[1])})\n",
    "rec_scores_df = pd.concat([rec_label_df, dfn1, dfn2], axis=1)\n",
    "print(rec_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append the tracks features (album, artist, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = data.get_tracks_df()\n",
    "rec_feature_track_df = rec_scores_df.join(tdf.set_index('track_id'), on='track_id')\n",
    "print(rec_feature_track_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm happy with the features gathered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_df = rec_feature_track_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    n = x['label'].sum()\n",
    "    ones = x.loc[x['label'] == 1]\n",
    "    zeros = x.loc[x['label'] == 0].sample(n)\n",
    "    return pd.concat([ones,zeros])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = feature_df.groupby(['playlist_id'], as_index=False).apply(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.reset_index().drop(['level_0', 'level_1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = data.get_target_playlists()\n",
    "train_tgt = random.sample(tgt, math.floor(len(tgt)*0.8))\n",
    "test_tgt = list(set(tgt) - set(train_tgt))\n",
    "train = full.loc[full['playlist_id'].isin(train_tgt)]\n",
    "test = full.loc[full['playlist_id'].isin(test_tgt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat_train = []\n",
    "to_concat_test = []\n",
    "to_onehot = []\n",
    "to_drop = ['album_id', 'artist_id', 'track_id', 'playlist_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotize(full, df, str):\n",
    "    exp = full[str].unique()\n",
    "    print(len(exp))\n",
    "    df.loc[:, (str)] = df[str].astype(CategoricalDtype(categories = exp))\n",
    "    oh = pd.get_dummies(df[str], prefix=str).to_sparse(fill_value=0)\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in to_onehot:\n",
    "    to_concat_train.append(onehotize(full, train, name))\n",
    "    to_concat_test.append(onehotize(full, test, name))\n",
    "    train = train.drop(name, axis=1)\n",
    "    test = test.drop(name, axis=1)\n",
    "to_concat_train.insert(0, train)\n",
    "to_concat_test.insert(0, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.concat(to_concat_train, axis=1)\n",
    "test = pd.concat(to_concat_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in to_drop:\n",
    "    train = train.drop(j, axis=1)\n",
    "    test = test.drop(j, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "label_train = train.label\n",
    "trainu = train.drop(['label'], axis=1)\n",
    "label_test = test.label\n",
    "testu = test.drop(['label'], axis=1)\n",
    "dtrain = xgb.DMatrix(trainu, label=label_train)\n",
    "dtest = xgb.DMatrix(testu, label=label_test)\n",
    "\n",
    "param = {'max_depth': 4, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic',\n",
    "         'lambda': 1, 'alpha': 1}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'\n",
    "\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "num_round = 200\n",
    "model_trained = xgb.train(param, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model_trained.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained.save_model('0001.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
